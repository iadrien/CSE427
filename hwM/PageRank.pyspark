# This is a collection of spark shell commands, NOT a spark program. 
# To run a .pyspark application first open the spark shell and then ues:
# exec file("PageRank.pyspark")
############################################################################

# given the list of neighbors for a page and that page's rank, calculate 
# what that page contributes to the rank of its neighbors
def computeContribs(neighbors, rank):
    for neighbor in neighbors: yield(neighbor, rank/len(neighbors))


# read in a file of page links (format: url1 url2)
# filter the results by preceding char e
linkfile="file:/home/cloudera/cse427s/hwM/california.txt"
links = sc.textFile(linkfile).map(lambda line: line.split())\
   .filter(lambda w: w[0]=='e')\
   .map(lambda pages: (pages[1],pages[2]))\
   .distinct()\
   .groupByKey()

# set initial page ranks to 1.0
ranks=links.map(lambda (page,neighbors): (page,1.0))

# number of iterations
n = 200

# for n iterations, calculate new page ranks based on neighbor contribibutios
for x in xrange(n):
  contribs=links\
    .join(ranks)\
    .flatMap(lambda (page,(neighbors,rank)): \
       computeContribs(neighbors,rank)) 
  ranks=contribs\
    .reduceByKey(lambda v1,v2: v1+v2)\
    .map(lambda (page,contrib): \
         (page,contrib * 0.85 + 0.15))

  if(x%50==0):
	ranks.checkpoint()

# get the results and save them
result = sorted(ranks.collect(), key=lambda x:x[1])

ranks.saveAsTextFile("file:/home/cloudera/cse427s/hwM/californiaresult")

sc.parallelize(result).saveAsTextFile("file:/home/cloudera/cse427s/hwM/californiaresultsorted")
